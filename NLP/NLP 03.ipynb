{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2d0a47a",
   "metadata": {},
   "source": [
    "# Text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d573d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      "He\n",
      "Hel\n",
      "Hell\n",
      "Hello\n",
      "Hello \n",
      "Hello h\n",
      "Hello he\n",
      "Hello he \n",
      "Hello he s\n",
      "Hello he sa\n",
      "Hello he sai\n",
      "Hello he said\n",
      "Hello he said \n",
      "Hello he said a\n",
      "Hello he said an\n",
      "Hello he said and\n",
      "Hello he said and \n",
      "Hello he said and w\n",
      "Hello he said and we\n",
      "Hello he said and wen\n",
      "Hello he said and went\n"
     ]
    }
   ],
   "source": [
    "punctuations = '''!()-{}[];:\"'\\,<>./?@#$%^&*_~`'''\n",
    "my_str = \"Hello!!!, he said ...and went.\"\n",
    "no_punct = \"\"\n",
    "for char in my_str:\n",
    "    if(char not in punctuations):\n",
    "        no_punct = no_punct + char\n",
    "        print(no_punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3093010d",
   "metadata": {},
   "source": [
    "# Regex expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24a72dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello he said and went\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "s = \"Hello!!!, he said ...and went.\"\n",
    "s = re.sub(r'[^\\w\\s]','',s)\n",
    "# not of word and space character\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e157695",
   "metadata": {},
   "source": [
    "# Part-of-Speech (PoS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b83e3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Yash Dhumal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import pos_tag\n",
    "text =\"she sells seashells on the seashore\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8593015a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['she', 'sells', 'seashells', 'on', 'the', 'seashore']\n",
      "After Token: [('she', 'PRP'), ('sells', 'VBZ'), ('seashells', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('seashore', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "print(\"After Split:\",text)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c4220",
   "metadata": {},
   "source": [
    "# Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "239689b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7e525c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "stop_words = set(stopwords.words('english')) # Set of English stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "299047a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(sentence) # Tokenize the sentence into words\n",
    "\n",
    "# Filtering the sentence\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7b71eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "Filtered tokens: ['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Original tokens:\", word_tokens)\n",
    "print(\"Filtered tokens:\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27388387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
