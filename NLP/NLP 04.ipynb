{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58a5f92-63d3-47a4-bf00-d4cd3eb1274b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall torch torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc5a73-eaba-4860-b50e-7cce6e29ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch==2.0.1 torchtext==0.15.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4453264-f2be-49bf-8d0e-528cfe4eb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data import Field, BucketIterator\n",
    "# Define the fields\n",
    "SRC = Field(tokenize=\"spacy\", tokenizer_language=\"en_core_web_sm\",\n",
    "init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n",
    "TRG = Field(tokenize=\"spacy\", tokenizer_language=\"de_core_news_sm\",\n",
    "init_token=\"<sos>\", eos_token=\"<eos>\", lower=True)\n",
    "# Load the dataset (using Multi30k for demonstration)\n",
    "train_data, valid_data, test_data = torchtext.datasets.Multi30k.splits(exts=(\".en\", \".de\"), fields=(SRC, TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab95e85-f456-473c-955a-55c055d6e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocab\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "# Create iterators\n",
    "BATCH_SIZE = 32\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "(train_data, valid_data, test_data), batch_size=BATCH_SIZE,\n",
    "device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f736f2ed-76ad-40b0-bffd-72899bb91cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "return hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7d241-cf8b-4474-99a3-0f8a74eb0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout,batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        def forward(self, input, hidden, cell):\n",
    "            input = input.unsqueeze(1) # Add batch dimension (for 1 timestep)\n",
    "            embedded = self.dropout(self.embedding(input))output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "            prediction = self.fc_out(output.squeeze(1))\n",
    "return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442bf091-da55-4bc1-9c4e-8708cd27658c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, trg_pad_idx, max_length=100):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.max_length = max_length\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # Tensor to hold the decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len,trg_vocab_size).to(self.device)\n",
    "        # Encoder forward pass\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # First input to the decoder is the <sos> token (start-of-sequencetoken)\n",
    "        input = trg[:, 0]\n",
    "        # Decoding\n",
    "    for t in range(1, trg_len):\n",
    "        output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "        outputs[:, t] = output\n",
    "        # Get the highest predicted token for the next time step\n",
    "        top1 = output.argmax(1)\n",
    "        # Decide if we are going to use teacher forcing or not\n",
    "        input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio \n",
    "    else\n",
    "        top1\n",
    "return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df91c809-d93e-4bde-983b-f204a0209381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "TRG_PAD_IDX = TRG.vocab.stoi['<pad>']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize the encoder, decoder, and seq2seq model\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT).to(device)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT).to(device)\n",
    "model = Seq2Seq(encoder, decoder, device, TRG_PAD_IDX).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 10\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch in train_iterator:\n",
    "        src = batch.src.to(device)\n",
    "        trg = batch.trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # Flatten the output and target tensors\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.view(-1, output_dim)\n",
    "        trg = trg.view(-1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_iterator):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2408cde8-32ce-4543-8837-3694fae4a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, src_vocab, trg_vocab, device, max_length=100):\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and convert the sentence to indices\n",
    "    tokens = [token.lower() for token in sentence.split()]\n",
    "    src_indices = [src_vocab.stoi[token] for token in tokens]\n",
    "    src_tensor = torch.tensor(src_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get encoder hidden and cell states\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    # Start with the <sos> token\n",
    "    trg_indices = [trg_vocab.stoi['<sos>']]\n",
    "\n",
    "    # Decode the sentence\n",
    "    for t in range(max_length):\n",
    "        trg_tensor = torch.tensor([trg_indices[-1]]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indices.append(pred_token)\n",
    "\n",
    "        # Stop if <eos> is predicted\n",
    "        if pred_token == trg_vocab.stoi['<eos>']:\n",
    "            break\n",
    "\n",
    "    # Convert indices back to tokens\n",
    "    trg_tokens = [trg_vocab.itos[idx] for idx in trg_indices]\n",
    "\n",
    "    return trg_tokens[1:]  # exclude <sos>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75c45cd-fa9b-4e28-b9bf-9983e52d2e90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f8028-3364-4daa-b64e-5f4c9d31e580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
